{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start as the Python Environment 'Python 3.9.6' is no longer available. Consider selecting another kernel or refreshing the list of Python Environments."
     ]
    }
   ],
   "source": [
    "import PyPDF2\n",
    "\n",
    "# Path to the PDF file\n",
    "pdf_path = 'sssv1.pdf'\n",
    "\n",
    "newfile = open(\"txtfile.txt\",\"w\")\n",
    "\n",
    "# Open the PDF file\n",
    "with open(pdf_path, 'rb') as file:\n",
    "    reader = PyPDF2.PdfReader(file)\n",
    "    num_pages = len(reader.pages)\n",
    "    \n",
    "    # Iterate through all pages and extract text\n",
    "    for page_num in range(num_pages):\n",
    "        page = reader.pages[page_num]\n",
    "        text = page.extract_text()\n",
    "        newfile.write(text)\n",
    "        #print(text)\n",
    "        #print(\"-\" * 40)\n",
    "newfile.close()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = open(\"txtfile.txt\").read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "370044"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sri Sathya Sai Speaks, Vol 1 (1953 - 60)\n",
      "Divine Discourses of Bhagawan Sri Sathya Sai BabaIndex Of Discourses\n",
      "1.   Have You Heard Our Baba Speak? ....................................................................... 2\n",
      "1.   Worship In The Mind .............................................................................................. 7\n",
      "2.   Total Surrender ...................................................................................................... 13\n",
      "3.   God As Guide ......................................................................................................... 20\n",
      "4.   Divine Life .............................................................................................................. 25\n",
      "5.   Meditation On The Lord's Name And Form ...................................................... 27\n",
      "6.   An Attitude Of Challenge ...................................................................................... 33\n",
      "7.   Courage ......................................\n"
     ]
    }
   ],
   "source": [
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stoi = {ch:i for i, ch in enumerate(chars)}\n",
    "itos = {i:ch for i,ch in enumerate(chars)}\n",
    "encode = lambda s: [stoi[ch] for ch in s]\n",
    "decode = lambda s: ''.join([itos[ch] for ch in s])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sairam'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode(encode(\"sairam\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/suriya/personal/swami-bot/venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[82, 958, 321]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "enc.encode(\"sairam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sairam'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc.decode([82,958,321])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "data = torch.tensor(encode(text),dtype=torch.long)\n",
    "n = int(0.9 * len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([42, 67, 58,  1, 42, 50, 69, 57, 74])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size=8\n",
    "train_data[:block_size+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "block_size = 8\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split==\"train\" else val_data\n",
    "    ix = torch.randint(len(data)-block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "    \n",
    "    def forward(self, idx, targets=None):\n",
    "        logits = self.token_embedding_table(idx)\n",
    "\n",
    "        if targets is None:\n",
    "            loss=None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(-1)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits, loss = self(idx)\n",
    "            logits = logits[:,-1,:]\n",
    "            probs = F.softmax(logits, dim=1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, idx_next),dim=1)\n",
    "        \n",
    "        return idx\n",
    "\n",
    "\n",
    "\n",
    "model = BigramLanguageModel(vocab_size=vocab_size)\n",
    "x, y = get_batch(\"train\")\n",
    "logits = model.forward(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\"i¦-oD vKâx\" ?IHl/fwQJa(\\'ceWFAKp50NF\\nSzS7hh-IUeNP/qTlqZmK\\nb;EyN1uBP[EoBvb10TL ba I/¦j9-bGy1p(8nw9:.!'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = torch.zeros((1,1),dtype=torch.long)\n",
    "decode(model.generate(idx,max_new_tokens=100)[0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(3.2938, grad_fn=<NllLossBackward0>)\n",
      "100 tensor(3.1281, grad_fn=<NllLossBackward0>)\n",
      "200 tensor(3.1281, grad_fn=<NllLossBackward0>)\n",
      "300 tensor(2.9321, grad_fn=<NllLossBackward0>)\n",
      "400 tensor(3.0231, grad_fn=<NllLossBackward0>)\n",
      "500 tensor(3.0287, grad_fn=<NllLossBackward0>)\n",
      "600 tensor(2.9190, grad_fn=<NllLossBackward0>)\n",
      "700 tensor(2.8607, grad_fn=<NllLossBackward0>)\n",
      "800 tensor(2.9269, grad_fn=<NllLossBackward0>)\n",
      "900 tensor(2.8112, grad_fn=<NllLossBackward0>)\n",
      "1000 tensor(2.8645, grad_fn=<NllLossBackward0>)\n",
      "1100 tensor(2.6287, grad_fn=<NllLossBackward0>)\n",
      "1200 tensor(2.7942, grad_fn=<NllLossBackward0>)\n",
      "1300 tensor(2.7157, grad_fn=<NllLossBackward0>)\n",
      "1400 tensor(2.9238, grad_fn=<NllLossBackward0>)\n",
      "1500 tensor(2.7871, grad_fn=<NllLossBackward0>)\n",
      "1600 tensor(2.8025, grad_fn=<NllLossBackward0>)\n",
      "1700 tensor(2.5992, grad_fn=<NllLossBackward0>)\n",
      "1800 tensor(2.6993, grad_fn=<NllLossBackward0>)\n",
      "1900 tensor(2.7128, grad_fn=<NllLossBackward0>)\n",
      "2000 tensor(2.6359, grad_fn=<NllLossBackward0>)\n",
      "2100 tensor(2.7479, grad_fn=<NllLossBackward0>)\n",
      "2200 tensor(2.7029, grad_fn=<NllLossBackward0>)\n",
      "2300 tensor(2.6973, grad_fn=<NllLossBackward0>)\n",
      "2400 tensor(2.7272, grad_fn=<NllLossBackward0>)\n",
      "2500 tensor(2.4650, grad_fn=<NllLossBackward0>)\n",
      "2600 tensor(2.4432, grad_fn=<NllLossBackward0>)\n",
      "2700 tensor(2.6687, grad_fn=<NllLossBackward0>)\n",
      "2800 tensor(2.6015, grad_fn=<NllLossBackward0>)\n",
      "2900 tensor(2.4784, grad_fn=<NllLossBackward0>)\n",
      "3000 tensor(2.5553, grad_fn=<NllLossBackward0>)\n",
      "3100 tensor(2.5960, grad_fn=<NllLossBackward0>)\n",
      "3200 tensor(2.5814, grad_fn=<NllLossBackward0>)\n",
      "3300 tensor(2.5801, grad_fn=<NllLossBackward0>)\n",
      "3400 tensor(2.6178, grad_fn=<NllLossBackward0>)\n",
      "3500 tensor(2.6382, grad_fn=<NllLossBackward0>)\n",
      "3600 tensor(2.5088, grad_fn=<NllLossBackward0>)\n",
      "3700 tensor(2.7188, grad_fn=<NllLossBackward0>)\n",
      "3800 tensor(2.6125, grad_fn=<NllLossBackward0>)\n",
      "3900 tensor(2.4330, grad_fn=<NllLossBackward0>)\n",
      "4000 tensor(2.2299, grad_fn=<NllLossBackward0>)\n",
      "4100 tensor(2.5356, grad_fn=<NllLossBackward0>)\n",
      "4200 tensor(2.5511, grad_fn=<NllLossBackward0>)\n",
      "4300 tensor(2.7133, grad_fn=<NllLossBackward0>)\n",
      "4400 tensor(2.4676, grad_fn=<NllLossBackward0>)\n",
      "4500 tensor(2.5580, grad_fn=<NllLossBackward0>)\n",
      "4600 tensor(2.5546, grad_fn=<NllLossBackward0>)\n",
      "4700 tensor(2.4741, grad_fn=<NllLossBackward0>)\n",
      "4800 tensor(2.6182, grad_fn=<NllLossBackward0>)\n",
      "4900 tensor(2.4778, grad_fn=<NllLossBackward0>)\n",
      "5000 tensor(2.5602, grad_fn=<NllLossBackward0>)\n",
      "5100 tensor(2.5162, grad_fn=<NllLossBackward0>)\n",
      "5200 tensor(2.4808, grad_fn=<NllLossBackward0>)\n",
      "5300 tensor(2.5587, grad_fn=<NllLossBackward0>)\n",
      "5400 tensor(2.5815, grad_fn=<NllLossBackward0>)\n",
      "5500 tensor(2.4622, grad_fn=<NllLossBackward0>)\n",
      "5600 tensor(2.5463, grad_fn=<NllLossBackward0>)\n",
      "5700 tensor(2.4705, grad_fn=<NllLossBackward0>)\n",
      "5800 tensor(2.5822, grad_fn=<NllLossBackward0>)\n",
      "5900 tensor(2.4562, grad_fn=<NllLossBackward0>)\n",
      "6000 tensor(2.4657, grad_fn=<NllLossBackward0>)\n",
      "6100 tensor(2.3696, grad_fn=<NllLossBackward0>)\n",
      "6200 tensor(2.4702, grad_fn=<NllLossBackward0>)\n",
      "6300 tensor(2.4011, grad_fn=<NllLossBackward0>)\n",
      "6400 tensor(2.5952, grad_fn=<NllLossBackward0>)\n",
      "6500 tensor(2.5404, grad_fn=<NllLossBackward0>)\n",
      "6600 tensor(2.5790, grad_fn=<NllLossBackward0>)\n",
      "6700 tensor(2.5142, grad_fn=<NllLossBackward0>)\n",
      "6800 tensor(2.5484, grad_fn=<NllLossBackward0>)\n",
      "6900 tensor(2.4522, grad_fn=<NllLossBackward0>)\n",
      "7000 tensor(2.3038, grad_fn=<NllLossBackward0>)\n",
      "7100 tensor(2.4556, grad_fn=<NllLossBackward0>)\n",
      "7200 tensor(2.5706, grad_fn=<NllLossBackward0>)\n",
      "7300 tensor(2.5246, grad_fn=<NllLossBackward0>)\n",
      "7400 tensor(2.3829, grad_fn=<NllLossBackward0>)\n",
      "7500 tensor(2.4829, grad_fn=<NllLossBackward0>)\n",
      "7600 tensor(2.3309, grad_fn=<NllLossBackward0>)\n",
      "7700 tensor(2.2335, grad_fn=<NllLossBackward0>)\n",
      "7800 tensor(2.4643, grad_fn=<NllLossBackward0>)\n",
      "7900 tensor(2.3938, grad_fn=<NllLossBackward0>)\n",
      "8000 tensor(2.2695, grad_fn=<NllLossBackward0>)\n",
      "8100 tensor(2.4445, grad_fn=<NllLossBackward0>)\n",
      "8200 tensor(2.5144, grad_fn=<NllLossBackward0>)\n",
      "8300 tensor(2.5911, grad_fn=<NllLossBackward0>)\n",
      "8400 tensor(2.4692, grad_fn=<NllLossBackward0>)\n",
      "8500 tensor(2.5718, grad_fn=<NllLossBackward0>)\n",
      "8600 tensor(2.7448, grad_fn=<NllLossBackward0>)\n",
      "8700 tensor(2.5372, grad_fn=<NllLossBackward0>)\n",
      "8800 tensor(2.4620, grad_fn=<NllLossBackward0>)\n",
      "8900 tensor(2.5036, grad_fn=<NllLossBackward0>)\n",
      "9000 tensor(2.3566, grad_fn=<NllLossBackward0>)\n",
      "9100 tensor(2.5227, grad_fn=<NllLossBackward0>)\n",
      "9200 tensor(2.3280, grad_fn=<NllLossBackward0>)\n",
      "9300 tensor(2.4947, grad_fn=<NllLossBackward0>)\n",
      "9400 tensor(2.4806, grad_fn=<NllLossBackward0>)\n",
      "9500 tensor(2.6618, grad_fn=<NllLossBackward0>)\n",
      "9600 tensor(2.3613, grad_fn=<NllLossBackward0>)\n",
      "9700 tensor(2.4210, grad_fn=<NllLossBackward0>)\n",
      "9800 tensor(2.4807, grad_fn=<NllLossBackward0>)\n",
      "9900 tensor(2.4633, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "for steps in range(10000):\n",
    "    x,y = get_batch(\"train\")\n",
    "    logits, loss = model.forward(x, y)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if steps%100==0:\n",
    "        print (steps, loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "I..\n",
      "8Sath lldaseat wisherive thinorre Gof bea stsexenteveande wher tean wn.se indn ste vingo cha thowe..sid 2.\n",
      "Sa trelethife a angakedarext is, \"RUna\n",
      "ste ny urst Pet alas mir s. aithemeally\n",
      "f arale werisisochencthe Plin totht, Dothev) I wasthatreshy I tive be de hire'istoomo n,\n",
      "f hat imy ne d tu tmicong ind'same e- on Gio adale.... Cure  rnan an d atheared ary ay o a f ope woripouthingr - vit \n",
      "ictaksake Ave secou tuas\n",
      "bus he ge,\n",
      "theh 1660 ghantipe inda aiveed; Whmene or th  the o sisiranoranotps\n"
     ]
    }
   ],
   "source": [
    "idx = torch.zeros((1,1),dtype=torch.long)\n",
    "print (decode(model.generate(idx,max_new_tokens=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
